digraph {
	graph [size="23.099999999999998,23.099999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1536726878640 [label="
 (1)" fillcolor=darkolivegreen1]
	1536726878400 [label="
 (1, 1)" fillcolor=darkolivegreen3]
	1536726878400 -> 1536726878640 [style=dotted]
	1536393893424 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	1536393343328 [label=AddmmBackward0]
	1536486915232 -> 1536393343328
	1535931309248 [label="value_net.bias
 (1)" fillcolor=lightblue]
	1535931309248 -> 1536486915232
	1536486915232 [label=AccumulateGrad]
	1536509863456 -> 1536393343328
	1536509863456 [label=TanhBackward0]
	1536509927216 -> 1536509863456
	1536509927216 [label=AddmmBackward0]
	1536509927360 -> 1536509927216
	1535931310608 [label="mlp_extractor.value_net.2.bias
 (64)" fillcolor=lightblue]
	1535931310608 -> 1536509927360
	1536509927360 [label=AccumulateGrad]
	1536509927552 -> 1536509927216
	1536509927552 [label=TanhBackward0]
	1536030396848 -> 1536509927552
	1536030396848 [label=AddmmBackward0]
	1536030396992 -> 1536030396848
	1535931310448 [label="mlp_extractor.value_net.0.bias
 (64)" fillcolor=lightblue]
	1535931310448 -> 1536030396992
	1536030396992 [label=AccumulateGrad]
	1536030396944 -> 1536030396848
	1536030396944 [label=CatBackward0]
	1536509519152 -> 1536030396944
	1536509519152 [label=ReluBackward0]
	1536030398144 -> 1536509519152
	1536030398144 [label=AddmmBackward0]
	1536030403760 -> 1536030398144
	1535931310848 [label="features_extractor.extractors.image.linear.0.bias
 (256)" fillcolor=lightblue]
	1535931310848 -> 1536030403760
	1536030403760 [label=AccumulateGrad]
	1536030398960 -> 1536030398144
	1536030398960 [label=UnsafeViewBackward0]
	1536487483440 -> 1536030398960
	1536487483440 [label=CloneBackward0]
	1536510470048 -> 1536487483440
	1536510470048 [label=ReluBackward0]
	1536404670592 -> 1536510470048
	1536404670592 [label=ConvolutionBackward0]
	1536510685728 -> 1536404670592
	1536510685728 [label=ReluBackward0]
	1536510684816 -> 1536510685728
	1536510684816 [label=ConvolutionBackward0]
	1536510684864 -> 1536510684816
	1536510684864 [label=ReluBackward0]
	1536510686064 -> 1536510684864
	1536510686064 [label=ConvolutionBackward0]
	1536510686256 -> 1536510686064
	1535931312208 [label="features_extractor.extractors.image.cnn.0.weight
 (32, 3, 8, 8)" fillcolor=lightblue]
	1535931312208 -> 1536510686256
	1536510686256 [label=AccumulateGrad]
	1536510686208 -> 1536510686064
	1535931312128 [label="features_extractor.extractors.image.cnn.0.bias
 (32)" fillcolor=lightblue]
	1535931312128 -> 1536510686208
	1536510686208 [label=AccumulateGrad]
	1536510685296 -> 1536510684816
	1535931311968 [label="features_extractor.extractors.image.cnn.2.weight
 (64, 32, 4, 4)" fillcolor=lightblue]
	1535931311968 -> 1536510685296
	1536510685296 [label=AccumulateGrad]
	1536510685776 -> 1536510684816
	1535931311888 [label="features_extractor.extractors.image.cnn.2.bias
 (64)" fillcolor=lightblue]
	1535931311888 -> 1536510685776
	1536510685776 [label=AccumulateGrad]
	1536510685920 -> 1536404670592
	1535931311328 [label="features_extractor.extractors.image.cnn.4.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1535931311328 -> 1536510685920
	1536510685920 [label=AccumulateGrad]
	1536510684720 -> 1536404670592
	1535931311248 [label="features_extractor.extractors.image.cnn.4.bias
 (64)" fillcolor=lightblue]
	1535931311248 -> 1536510684720
	1536510684720 [label=AccumulateGrad]
	1536030394208 -> 1536030398144
	1536030394208 [label=TBackward0]
	1536510468416 -> 1536030394208
	1535931310928 [label="features_extractor.extractors.image.linear.0.weight
 (256, 1920)" fillcolor=lightblue]
	1535931310928 -> 1536510468416
	1536510468416 [label=AccumulateGrad]
	1536030396800 -> 1536030396848
	1536030396800 [label=TBackward0]
	1536030401456 -> 1536030396800
	1535931310768 [label="mlp_extractor.value_net.0.weight
 (64, 259)" fillcolor=lightblue]
	1535931310768 -> 1536030401456
	1536030401456 [label=AccumulateGrad]
	1536030404624 -> 1536509927216
	1536030404624 [label=TBackward0]
	1536030393872 -> 1536030404624
	1535931310528 [label="mlp_extractor.value_net.2.weight
 (64, 64)" fillcolor=lightblue]
	1535931310528 -> 1536030393872
	1536030393872 [label=AccumulateGrad]
	1536486548352 -> 1536393343328
	1536486548352 [label=TBackward0]
	1536509935136 -> 1536486548352
	1535931309648 [label="value_net.weight
 (1, 64)" fillcolor=lightblue]
	1535931309648 -> 1536509935136
	1536509935136 [label=AccumulateGrad]
	1536393343328 -> 1536393893424
	1536726878560 [label="
 (1)" fillcolor=darkolivegreen1]
	1536407849520 [label=SqueezeBackward1]
	1536030394160 -> 1536407849520
	1536030394160 [label=GatherBackward0]
	1536030393920 -> 1536030394160
	1536030393920 [label=SubBackward0]
	1536030394256 -> 1536030393920
	1536030394256 [label=AddmmBackward0]
	1536510686160 -> 1536030394256
	1535931309328 [label="action_net.bias
 (7)" fillcolor=lightblue]
	1535931309328 -> 1536510686160
	1536510686160 [label=AccumulateGrad]
	1536510684528 -> 1536030394256
	1536510684528 [label=TanhBackward0]
	1536510686400 -> 1536510684528
	1536510686400 [label=AddmmBackward0]
	1536510686496 -> 1536510686400
	1535931310688 [label="mlp_extractor.policy_net.2.bias
 (64)" fillcolor=lightblue]
	1535931310688 -> 1536510686496
	1536510686496 [label=AccumulateGrad]
	1536510686448 -> 1536510686400
	1536510686448 [label=TanhBackward0]
	1536510686640 -> 1536510686448
	1536510686640 [label=AddmmBackward0]
	1536510686784 -> 1536510686640
	1535931311088 [label="mlp_extractor.policy_net.0.bias
 (64)" fillcolor=lightblue]
	1535931311088 -> 1536510686784
	1536510686784 [label=AccumulateGrad]
	1536030396944 -> 1536510686640
	1536510686736 -> 1536510686640
	1536510686736 [label=TBackward0]
	1536510686880 -> 1536510686736
	1535931310368 [label="mlp_extractor.policy_net.0.weight
 (64, 259)" fillcolor=lightblue]
	1535931310368 -> 1536510686880
	1536510686880 [label=AccumulateGrad]
	1536510686304 -> 1536510686400
	1536510686304 [label=TBackward0]
	1536510686832 -> 1536510686304
	1535931311008 [label="mlp_extractor.policy_net.2.weight
 (64, 64)" fillcolor=lightblue]
	1535931311008 -> 1536510686832
	1536510686832 [label=AccumulateGrad]
	1536510685872 -> 1536030394256
	1536510685872 [label=TBackward0]
	1536510686688 -> 1536510685872
	1535931309408 [label="action_net.weight
 (7, 64)" fillcolor=lightblue]
	1535931309408 -> 1536510686688
	1536510686688 [label=AccumulateGrad]
	1536510685488 -> 1536030393920
	1536510685488 [label=LogsumexpBackward0]
	1536030394256 -> 1536510685488
	1536407849520 -> 1536726878560
	1536726878240 [label="
 (1, 1)" fillcolor=darkolivegreen3]
	1536030394160 -> 1536726878240
	1536726878240 -> 1536726878560 [style=dotted]
}
